import pandas as pd

from multiprocessing import Pool

from gensim.utils import simple_preprocess
import gensim

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
stop_words_esp = stopwords.words('spanish')

################################################################################################################
################################################################################################################

## "sentences" = column of the dataframe with the content of the articles // "cores" = number of cores in the computer used (to make the process faster)
def cleaning_data(sentences, cores):
    data_words = []
    with Pool(processes=cores) as pool:
        ## Clean the data (lowercases, tokenizes, de-accents)
        for sentence in sentences:
            x = gensim.utils.simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations
            if x not in stop_words:
                data_words.append(x)

        ## Remove all the stopwords
        data_words = [[word for word in doc if word not in stop_words] for doc in data_words]
 
        ## Make bigrams (if bigrams are not necessary, remove this part)
        bigram = gensim.models.Phrases(data_words, min_count=500, threshold=750)
        bigram_mod = gensim.models.phrases.Phraser(bigram)

        return [bigram_mod[doc] for doc in data_words]
